# Neural Network from Scratch for MNIST Classification

This project implements a single hidden layer neural network from scratch using Python to classify handwritten digits from the MNIST dataset. It explores the underlying mathematics of neural networks, including forward and backward propagation with a focus on linear algebra and calculus. This approach is a deep dive into the workings of neural networks, making it ideal for those interested in understanding the core mechanisms.

## Project Overview

The project includes:
- **Data Loading**: Loading and preparing the MNIST dataset.
- **Neural Network Architecture**: A single hidden layer neural network designed for multiclass classification.
- **Activation Functions**: Utilization of softmax in the output layer for multiclass predictions.
- **Backpropagation**: Implementation of backpropagation from scratch, including the calculations of gradients for weight updates.

### Key Concepts Covered
- **Forward Propagation**: Matrix operations and activation functions.
- **Backpropagation**: Gradient computation and weight updates.
- **Softmax Activation**: Detailed implementation of softmax to handle multiclass outputs.
  
### Prerequisites
- Python 3.x
- Libraries: `numpy`, `matplotlib`

### Usage
Run the Jupyter notebook file to see each step in the neural network implementation and the training process for the MNIST dataset.

### Acknowledgments
This project was created to deepen understanding of neural network fundamentals and backpropagation calculus by implementing everything from scratch.
